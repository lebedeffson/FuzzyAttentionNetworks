#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
"""

import os
import shutil
from pathlib import Path

def clean_project():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ç –Ω–µ–Ω—É–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    print("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞...")
    
    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ñ–∞–π–ª—ã
    files_to_remove = [
        'run_web_interface.py',  # –°—Ç–∞—Ä—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        'demos/final_web_interface.py',  # –°—Ç–∞—Ä—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
        'models/best_cifar10_fan_model.pth',  # –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å
        'models/best_improved_cifar10_fan_model.pth',  # –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å
    ]
    
    for file_path in files_to_remove:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"  ‚ùå –£–¥–∞–ª–µ–Ω: {file_path}")
    
    # –û—á–∏—â–∞–µ–º –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏
    empty_dirs = ['datasets', 'results']
    for dir_path in empty_dirs:
        if os.path.exists(dir_path) and not os.listdir(dir_path):
            os.rmdir(dir_path)
            print(f"  ‚ùå –£–¥–∞–ª–µ–Ω–∞ –ø—É—Å—Ç–∞—è –ø–∞–ø–∫–∞: {dir_path}")
    
    print("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

def create_final_structure():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    print("üìÅ –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    dirs_to_create = [
        'docs',
        'examples',
        'scripts'
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"  ‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {dir_path}")
    
    print("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–∑–¥–∞–Ω–∞!")

def create_example_usage():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    print("üìù –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è...")
    
    example_code = '''#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è FAN –º–æ–¥–µ–ª–µ–π
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from dataset_manager import DatasetManager
from universal_fan_model import ModelManager
from transformers import BertTokenizer
import torch
from PIL import Image
import torchvision.transforms as transforms

def main():
    """–ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã —Å FAN –º–æ–¥–µ–ª—è–º–∏"""
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    dataset_manager = DatasetManager()
    model_manager = ModelManager()
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    # –í—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_name = 'cifar10'  # –∏–ª–∏ 'hateful_memes'
    
    print(f"üéØ –†–∞–±–æ—Ç–∞ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_name}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset = dataset_manager.create_dataset(dataset_name, 'train')
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = model_manager.get_model(dataset_name)
    print("ü§ñ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    text = "This is a sample text for testing"
    image = Image.new('RGB', (224, 224), color='blue')
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    text_tokens = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=64,
        return_tensors='pt'
    )
    
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    image_tensor = transform(image).unsqueeze(0)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    with torch.no_grad():
        result = model_manager.predict(
            dataset_name,
            text_tokens['input_ids'],
            text_tokens['attention_mask'],
            image_tensor,
            return_explanations=True
        )
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    prediction = result['predictions'].item()
    confidence = result['confidence'].item()
    
    print(f"üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {dataset.class_names[prediction]}")
    print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
    
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    probs = result['probs'].cpu().numpy()[0]
    print("\\nüìà –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for i, (class_name, prob) in enumerate(zip(dataset.class_names, probs)):
        print(f"  {class_name}: {prob:.3f}")

if __name__ == "__main__":
    main()
'''
    
    with open('examples/basic_usage.py', 'w', encoding='utf-8') as f:
        f.write(example_code)
    
    print("  ‚úÖ –°–æ–∑–¥–∞–Ω: examples/basic_usage.py")

def create_documentation():
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    print("üìö –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
    arch_doc = '''# üèóÔ∏è FAN Architecture Documentation

## Overview
Fuzzy Attention Networks (FAN) implement a novel approach to multimodal learning using fuzzy logic principles for attention mechanisms.

## Key Components

### 1. Universal FAN Model
- **File**: `src/universal_fan_model.py`
- **Purpose**: Core model architecture supporting multiple datasets
- **Features**: 
  - Configurable for different datasets
  - Transfer learning with BERT and ResNet
  - Fuzzy attention mechanisms

### 2. Dataset Manager
- **File**: `src/dataset_manager.py`
- **Purpose**: Unified interface for dataset handling
- **Features**:
  - Support for multiple datasets
  - Automatic data loading and preprocessing
  - Balanced sampling for class imbalance

### 3. Web Interface
- **File**: `demos/universal_web_interface.py`
- **Purpose**: Interactive web application
- **Features**:
  - Dataset selection
  - Real-time predictions
  - Interpretability visualizations

## Model Variants

### Hateful Memes Model
- **Architecture**: BERT + ResNet50 + 8-Head FAN
- **Performance**: F1: 0.5649, Accuracy: 59%
- **Use Case**: Binary classification of hateful content

### CIFAR-10 Model
- **Architecture**: BERT + ResNet18 + 4-Head FAN
- **Performance**: F1: 0.8808, Accuracy: 85%
- **Use Case**: 10-class image classification

## Fuzzy Attention Mechanism

The core innovation is the use of fuzzy membership functions for attention:

1. **Bell-shaped Functions**: Soft attention boundaries
2. **Learnable Parameters**: Centers and widths
3. **Multi-head Architecture**: Parallel attention heads
4. **Interpretability**: Human-readable patterns

## Usage

See `examples/basic_usage.py` for detailed usage examples.
'''
    
    with open('docs/architecture.md', 'w', encoding='utf-8') as f:
        f.write(arch_doc)
    
    print("  ‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: docs/architecture.md")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏"""
    print("üöÄ –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞ FAN...")
    print("=" * 50)
    
    # –û—á–∏—Å—Ç–∫–∞
    clean_project()
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞
    create_final_structure()
    
    # –ü—Ä–∏–º–µ—Ä—ã
    create_example_usage()
    
    # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    create_documentation()
    
    print("=" * 50)
    print("‚úÖ –ü—Ä–æ–µ–∫—Ç —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
    print("üéØ –ì–æ—Ç–æ–≤ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
    print("üåê –ó–∞–ø—É—Å–∫: python run_universal_interface.py")

if __name__ == "__main__":
    main()

