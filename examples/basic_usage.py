#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è FAN –º–æ–¥–µ–ª–µ–π
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from dataset_manager import DatasetManager
from universal_fan_model import ModelManager
from transformers import BertTokenizer
import torch
from PIL import Image
import torchvision.transforms as transforms

def main():
    """–ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã —Å FAN –º–æ–¥–µ–ª—è–º–∏"""
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    dataset_manager = DatasetManager()
    model_manager = ModelManager()
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    # –í—ã–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_name = 'cifar10'  # –∏–ª–∏ 'hateful_memes'
    
    print(f"üéØ –†–∞–±–æ—Ç–∞ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º: {dataset_name}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset = dataset_manager.create_dataset(dataset_name, 'train')
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = model_manager.get_model(dataset_name)
    print("ü§ñ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    text = "This is a sample text for testing"
    image = Image.new('RGB', (224, 224), color='blue')
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    text_tokens = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=64,
        return_tensors='pt'
    )
    
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    image_tensor = transform(image).unsqueeze(0)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    with torch.no_grad():
        result = model_manager.predict(
            dataset_name,
            text_tokens['input_ids'],
            text_tokens['attention_mask'],
            image_tensor,
            return_explanations=True
        )
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    prediction = result['predictions'].item()
    confidence = result['confidence'].item()
    
    print(f"üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {dataset.class_names[prediction]}")
    print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
    
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    probs = result['probs'].cpu().numpy()[0]
    print("\nüìà –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for i, (class_name, prob) in enumerate(zip(dataset.class_names, probs)):
        print(f"  {class_name}: {prob:.3f}")

if __name__ == "__main__":
    main()
