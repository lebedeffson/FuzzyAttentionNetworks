#!/usr/bin/env python3
"""
Ensemble Light Models Training Script
Ensemble –∏–∑ –ª–µ–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
"""

import os
import json
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np
from tqdm import tqdm
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import matplotlib.pyplot as plt

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üöÄ Training Ensemble Light Models")
print("=" * 50)
print(f"Using device: {DEVICE}")

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs('models', exist_ok=True)
os.makedirs('results', exist_ok=True)

class LightMemeDataset(Dataset):
    """–õ–µ–≥–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å –ø—Ä–æ—Å—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    
    def __init__(self, data, max_length=64, is_training=True):
        self.data = data
        self.max_length = max_length
        self.is_training = is_training
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.image_transform = transforms.Compose([
            transforms.Resize((64, 64)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text = item.get('text', '')
        if not text:
            text = item.get('question', '')
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words = text.split()[:self.max_length]
        text_tokens = [hash(word) % 10000 for word in words]
        text_tokens = text_tokens + [0] * (self.max_length - len(text_tokens))
        text_tokens = text_tokens[:self.max_length]
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_path = item.get('img', '')
        if not image_path:
            image_path = item.get('image_path', '')
        
        try:
            if os.path.exists(image_path):
                image = Image.open(image_path).convert('RGB')
            else:
                image = Image.new('RGB', (64, 64), color=(128, 128, 128))
        except:
            image = Image.new('RGB', (64, 64), color=(128, 128, 128))
        
        image = self.image_transform(image)
        
        # –õ–µ–π–±–ª
        label = int(item.get('label', 0))
        
        return {
            'text_tokens': torch.tensor(text_tokens, dtype=torch.long),
            'image': image,
            'label': label,
            'text': text
        }

class LightMemeModel(nn.Module):
    """–õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–µ–º–æ–≤"""
    
    def __init__(self, vocab_size=10000, text_dim=128, image_dim=12288, 
                 hidden_dim=256, num_classes=2, model_id=0):
        super().__init__()
        
        self.model_id = model_id
        
        # Text encoder (–ø—Ä–æ—Å—Ç–æ–π)
        self.text_embedding = nn.Embedding(vocab_size, text_dim)
        self.text_encoder = nn.LSTM(text_dim, hidden_dim, batch_first=True)
        
        # Image encoder (–ø—Ä–æ—Å—Ç–æ–π CNN)
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # Fuzzy attention (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        self.fuzzy_attention = SimpleFuzzyAttention(hidden_dim)
        
        # Classification head
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, num_classes)
        )
        
    def forward(self, text_tokens, image):
        batch_size = text_tokens.size(0)
        
        # Text encoding
        text_emb = self.text_embedding(text_tokens)
        text_output, _ = self.text_encoder(text_emb)
        text_features = text_output.mean(dim=1)
        
        # Image encoding
        image_features = self.image_encoder(image)
        image_features = image_features.view(batch_size, -1)
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ hidden_dim
        image_proj = nn.Linear(128, text_features.size(1)).to(image.device)
        image_features = image_proj(image_features)
        
        # Fuzzy attention
        text_attended = self.fuzzy_attention(text_features)
        
        # Fusion
        combined = torch.cat([text_attended, image_features], dim=-1)
        
        # Classification
        logits = self.classifier(combined)
        probs = torch.softmax(logits, dim=-1)
        
        return {
            'logits': logits,
            'probs': probs,
            'predictions': torch.argmax(logits, dim=-1),
            'confidence': torch.max(probs, dim=-1)[0]
        }

class SimpleFuzzyAttention(nn.Module):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π fuzzy attention"""
    
    def __init__(self, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # Fuzzy membership functions
        self.fuzzy_centers = nn.Parameter(torch.randn(3))
        self.fuzzy_widths = nn.Parameter(torch.ones(3))
        
        # Attention weights
        self.attention_weights = nn.Linear(hidden_dim, 1)
        
    def forward(self, x):
        # –ü—Ä–æ—Å—Ç–æ–π fuzzy attention
        attention_scores = self.attention_weights(x)
        
        # Fuzzy membership
        fuzzy_scores = torch.zeros_like(attention_scores)
        for i in range(3):
            center = self.fuzzy_centers[i]
            width = self.fuzzy_widths[i]
            membership = torch.exp(-((attention_scores - center) ** 2) / (2 * width ** 2))
            fuzzy_scores += membership
        
        # Normalize
        fuzzy_scores = fuzzy_scores / 3.0
        
        # Apply attention
        attended = x * fuzzy_scores
        
        return attended

class EnsembleLightModels(nn.Module):
    """Ensemble –∏–∑ –ª–µ–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, num_models=3, vocab_size=10000, text_dim=128, 
                 image_dim=12288, hidden_dim=256, num_classes=2):
        super().__init__()
        
        self.num_models = num_models
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π
        self.models = nn.ModuleList([
            LightMemeModel(vocab_size, text_dim, image_dim, hidden_dim, num_classes, i)
            for i in range(num_models)
        ])
        
        # Ensemble classifier
        self.ensemble_classifier = nn.Sequential(
            nn.Linear(num_models * num_classes, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, num_classes)
        )
        
    def forward(self, text_tokens, image):
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        all_logits = []
        all_probs = []
        
        for model in self.models:
            outputs = model(text_tokens, image)
            all_logits.append(outputs['logits'])
            all_probs.append(outputs['probs'])
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        combined_logits = torch.cat(all_logits, dim=-1)
        ensemble_logits = self.ensemble_classifier(combined_logits)
        ensemble_probs = torch.softmax(ensemble_logits, dim=-1)
        
        return {
            'logits': ensemble_logits,
            'probs': ensemble_probs,
            'predictions': torch.argmax(ensemble_logits, dim=-1),
            'confidence': torch.max(ensemble_probs, dim=-1)[0],
            'individual_logits': all_logits,
            'individual_probs': all_probs
        }

def load_dataset():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç"""
    print("üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...")
    
    with open('data/hateful_memes/train.jsonl', 'r') as f:
        data = [json.loads(line) for line in f]
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    random.shuffle(data)
    split_idx = int(0.8 * len(data))
    train_data = data[:split_idx]
    val_data = data[split_idx:]
    
    print(f"üìä Train: {len(train_data)}, Val: {len(val_data)}")
    
    return train_data, val_data

def collate_fn(batch):
    """Collate function for DataLoader"""
    text_tokens = torch.stack([item['text_tokens'] for item in batch])
    images = torch.stack([item['image'] for item in batch])
    labels = torch.tensor([item['label'] for item in batch])
    texts = [item['text'] for item in batch]
    
    return {
        'text_tokens': text_tokens,
        'image': images,
        'label': labels,
        'text': texts
    }

def train_epoch(model, dataloader, optimizer, criterion, device):
    """–û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è"""
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    for batch in tqdm(dataloader, desc="Training"):
        text_tokens = batch['text_tokens'].to(device)
        images = batch['image'].to(device)
        labels = batch['label'].to(device)
        
        optimizer.zero_grad()
        
        outputs = model(text_tokens, images)
        loss = criterion(outputs['logits'], labels)
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        preds = outputs['predictions'].detach().cpu().numpy()
        labels_np = labels.detach().cpu().numpy()
        
        all_preds.extend(preds)
        all_labels.extend(labels_np)
    
    avg_loss = total_loss / len(dataloader)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    
    return avg_loss, f1

def evaluate(model, dataloader, criterion, device):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            text_tokens = batch['text_tokens'].to(device)
            images = batch['image'].to(device)
            labels = batch['label'].to(device)
            
            outputs = model(text_tokens, images)
            loss = criterion(outputs['logits'], labels)
            
            total_loss += loss.item()
            
            preds = outputs['predictions'].detach().cpu().numpy()
            labels_np = labels.detach().cpu().numpy()
            probs = outputs['probs'].detach().cpu().numpy()
            
            all_preds.extend(preds)
            all_labels.extend(labels_np)
            all_probs.extend(probs)
    
    avg_loss = total_loss / len(dataloader)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    precision = precision_score(all_labels, all_preds, average='weighted')
    recall = recall_score(all_labels, all_preds, average='weighted')
    accuracy = accuracy_score(all_labels, all_preds)
    
    return avg_loss, f1, precision, recall, accuracy

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    train_data, val_data = load_dataset()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = LightMemeDataset(train_data, is_training=True)
    val_dataset = LightMemeDataset(val_data, is_training=False)
    
    # DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)
    
    # Ensemble –º–æ–¥–µ–ª—å
    model = EnsembleLightModels(num_models=3).to(DEVICE)
    print(f"Ensemble model created with {sum(p.numel() for p in model.parameters()):,} parameters")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ loss
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # –û–±—É—á–µ–Ω–∏–µ
    best_f1 = 0
    patience = 5
    patience_counter = 0
    
    for epoch in range(15):  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è ensemble
        print(f"\nEpoch {epoch+1}/15")
        print("-" * 30)
        
        # Train
        train_loss, train_f1 = train_epoch(model, train_loader, optimizer, criterion, DEVICE)
        
        # Val
        val_loss, val_f1, val_precision, val_recall, val_accuracy = evaluate(
            model, val_loader, criterion, DEVICE
        )
        
        print(f"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}")
        print(f"Val Accuracy: {val_accuracy:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if val_f1 > best_f1:
            best_f1 = val_f1
            patience_counter = 0
            torch.save(model.state_dict(), 'models/best_ensemble_light_model.pth')
            print(f"‚úÖ New best model saved! (Val F1: {val_f1:.4f})")
        else:
            patience_counter += 1
        
        # Early stopping
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    print("\nüìä Final Evaluation...")
    model.load_state_dict(torch.load('models/best_ensemble_light_model.pth'))
    val_loss, val_f1, val_precision, val_recall, val_accuracy = evaluate(
        model, val_loader, criterion, DEVICE
    )
    
    print(f"\nüìà Final Results:")
    print(f"   Accuracy: {val_accuracy:.4f}")
    print(f"   F1 Score: {val_f1:.4f}")
    print(f"   Precision: {val_precision:.4f}")
    print(f"   Recall: {val_recall:.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        'accuracy': val_accuracy,
        'f1_score': val_f1,
        'precision': val_precision,
        'recall': val_recall,
        'epochs': epoch + 1,
        'best_f1': best_f1,
        'model_type': 'ensemble_light',
        'num_models': 3
    }
    
    with open('results/ensemble_light_model_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    if val_f1 >= 0.70:
        print("üéâ Target reached! F1 >= 0.70")
    else:
        print(f"‚ö†Ô∏è Target not reached. F1 = {val_f1:.4f} < 0.70")
    
    print(f"\nüéâ Ensemble light model training completed!")
    print(f"Best model saved to: models/best_ensemble_light_model.pth")
    print(f"Results saved to: results/ensemble_light_model_results.json")

if __name__ == "__main__":
    main()
