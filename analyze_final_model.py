#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ best_advanced_metrics_model.pth
"""

import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
import sys
import os
import math
from transformers import BertTokenizer, BertModel
import torchvision.models as models

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

class AdvancedFuzzyAttention(nn.Module):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è fuzzy attention —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self, hidden_dim, num_heads=8, num_membership=7):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        self.num_membership = num_membership
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ fuzzy membership functions
        self.fuzzy_centers = nn.Parameter(torch.randn(num_heads, num_membership, self.head_dim) * 0.05)
        self.fuzzy_widths = nn.Parameter(torch.ones(num_heads, num_membership, self.head_dim) * 0.2)
        
        # Multi-scale attention
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, hidden_dim)
        
        # Residual connection
        self.norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.1)
        
        # Attention gating
        self.attention_gate = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()
        )
        
    def bell_membership(self, x, center, width):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–ª–æ–∫–æ–ª–æ–æ–±—Ä–∞–∑–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏"""
        return 1 / (1 + ((x - center) / (width + 1e-8)) ** 2)
        
    def forward(self, query, key, value, return_interpretation=False):
        batch_size = query.size(0)
        residual = query
        
        # Linear projections
        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # Apply fuzzy membership functions
        fuzzy_scores = torch.zeros_like(scores)
        membership_values = {}
        
        for h in range(self.num_heads):
            for f in range(self.num_membership):
                center = self.fuzzy_centers[h, f].unsqueeze(0).unsqueeze(0)
                width = self.fuzzy_widths[h, f].unsqueeze(0).unsqueeze(0)
                
                # Bell membership function
                membership = self.bell_membership(scores[:, h], center, width)
                fuzzy_scores[:, h] += membership.mean(dim=-1, keepdim=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
                if return_interpretation:
                    membership_values[f'head_{h}_func_{f}'] = {
                        'center': center,
                        'width': width,
                        'membership': membership,
                        'contribution': membership.mean(dim=-1, keepdim=True)
                    }
        
        # Normalize
        fuzzy_scores = fuzzy_scores / self.num_membership
        
        # Apply softmax
        attention_weights = torch.softmax(fuzzy_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attended = torch.matmul(attention_weights, V)
        
        # Concatenate heads
        attended = attended.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)
        
        # Output projection
        output = self.output(attended)
        
        # Attention gating
        gate = self.attention_gate(output)
        output = output * gate
        
        # Residual connection
        output = self.norm(output + residual)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        if return_interpretation:
            self.attention_weights = attention_weights
            self.fuzzy_scores = fuzzy_scores
            self.membership_values = membership_values
        
        return output, attention_weights

class AdvancedFANModel(nn.Module):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è FAN –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self, num_classes=2, num_heads=8, hidden_dim=768):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # BERT –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å fine-tuning
        self.bert_model = BertModel.from_pretrained('bert-base-uncased')
        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ BERT
        for param in self.bert_model.encoder.layer[-2:].parameters():
            param.requires_grad = True
        
        # ResNet –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å fine-tuning
        self.resnet = models.resnet50(pretrained=True)
        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ ResNet
        for param in self.resnet.layer4.parameters():
            param.requires_grad = True
        
        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, hidden_dim)
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ Fuzzy Attention Networks
        self.text_fuzzy_attention = AdvancedFuzzyAttention(hidden_dim, num_heads, 7)
        self.image_fuzzy_attention = AdvancedFuzzyAttention(hidden_dim, num_heads, 7)
        
        # Multi-scale cross-modal attention
        self.cross_modal_attention = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=0.1)
        
        # Advanced fusion layers
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_dim)
        )
        
        # Advanced classification head
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, text_tokens, attention_mask, image, return_explanations=False):
        batch_size = text_tokens.size(0)
        
        # BERT encoding –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        bert_outputs = self.bert_model(text_tokens, attention_mask=attention_mask)
        text_features = bert_outputs.last_hidden_state.mean(dim=1)
        
        # ResNet encoding –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_features = self.resnet(image)
        
        # Fuzzy attention –Ω–∞ —Ç–µ–∫—Å—Ç–µ
        text_attended, text_attention_weights = self.text_fuzzy_attention(
            text_features.unsqueeze(1), text_features.unsqueeze(1), text_features.unsqueeze(1),
            return_interpretation=return_explanations
        )
        text_attended = text_attended.squeeze(1)
        
        # Fuzzy attention –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
        image_attended, image_attention_weights = self.image_fuzzy_attention(
            image_features.unsqueeze(1), image_features.unsqueeze(1), image_features.unsqueeze(1),
            return_interpretation=return_explanations
        )
        image_attended = image_attended.squeeze(1)
        
        # Cross-modal attention
        text_enhanced, cross_modal_weights = self.cross_modal_attention(
            text_attended.unsqueeze(1), image_attended.unsqueeze(1), image_attended.unsqueeze(1)
        )
        text_enhanced = text_enhanced.squeeze(1)
        
        # Fusion
        combined = torch.cat([text_enhanced, image_attended], dim=1)
        fused = self.fusion_layer(combined)
        
        # Classification
        logits = self.classifier(fused)
        probs = torch.softmax(logits, dim=1)
        
        result = {
            'logits': logits,
            'probs': probs,
            'predictions': torch.argmax(logits, dim=1),
            'confidence': torch.max(probs, dim=1)[0]
        }
        
        if return_explanations:
            result['explanations'] = {
                'text_attention': text_attention_weights,
                'image_attention': image_attention_weights,
                'cross_modal_attention': cross_modal_weights,
                'text_fuzzy_membership': self.text_fuzzy_attention.membership_values,
                'image_fuzzy_membership': self.image_fuzzy_attention.membership_values,
                'text_features': text_features,
                'image_features': image_features
            }
            
        return result

def analyze_final_model():
    """–ê–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    print("üîç –ê–ù–ê–õ–ò–ó –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò: best_advanced_metrics_model.pth")
    print("=" * 80)
    
    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model_path = "models/best_advanced_metrics_model.pth"
    if not Path(model_path).exists():
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        return
    
    print(f"üìÅ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {Path(model_path).stat().st_size / (1024*1024):.1f} MB")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = AdvancedFANModel(
        num_classes=2,
        num_heads=8,
        hidden_dim=768
    ).to(device)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        model.load_state_dict(torch.load(model_path, map_location=device))
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return
    
    print("\nüèóÔ∏è –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò:")
    print("-" * 50)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"üéØ –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params/total_params*100:.1f}%")
    
    print("\nüß† –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ê–†–•–ò–¢–ï–ö–¢–£–†–´:")
    print("-" * 50)
    
    # BERT
    bert_params = sum(p.numel() for p in model.bert_model.parameters())
    bert_trainable = sum(p.numel() for p in model.bert_model.parameters() if p.requires_grad)
    print(f"üìù BERT (text encoder):")
    print(f"   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {bert_params:,}")
    print(f"   - –û–±—É—á–∞–µ–º—ã—Ö: {bert_trainable:,}")
    print(f"   - Fine-tuning: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —Å–ª–æ—è —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω—ã")
    
    # ResNet
    resnet_params = sum(p.numel() for p in model.resnet.parameters())
    resnet_trainable = sum(p.numel() for p in model.resnet.parameters() if p.requires_grad)
    print(f"üñºÔ∏è ResNet50 (image encoder):")
    print(f"   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {resnet_params:,}")
    print(f"   - –û–±—É—á–∞–µ–º—ã—Ö: {resnet_trainable:,}")
    print(f"   - Fine-tuning: layer4 —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω")
    
    # Fuzzy Attention
    text_fuzzy_params = sum(p.numel() for p in model.text_fuzzy_attention.parameters())
    image_fuzzy_params = sum(p.numel() for p in model.image_fuzzy_attention.parameters())
    print(f"üé≠ Fuzzy Attention Networks:")
    print(f"   - Text FAN: {text_fuzzy_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print(f"   - Image FAN: {image_fuzzy_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print(f"   - –ì–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è: {model.num_heads}")
    print(f"   - –§—É–Ω–∫—Ü–∏–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏: 7 –Ω–∞ –≥–æ–ª–æ–≤—É")
    print(f"   - –í—Å–µ–≥–æ fuzzy —Ñ—É–Ω–∫—Ü–∏–π: {model.num_heads * 7 * 2} (text + image)")
    
    # Cross-modal attention
    cross_modal_params = sum(p.numel() for p in model.cross_modal_attention.parameters())
    print(f"üîó Cross-modal Attention: {cross_modal_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    # Fusion layers
    fusion_params = sum(p.numel() for p in model.fusion_layer.parameters())
    print(f"üîÄ Fusion Layers: {fusion_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    # Classifier
    classifier_params = sum(p.numel() for p in model.classifier.parameters())
    print(f"üéØ Classifier: {classifier_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print(f"   - –°–ª–æ–µ–≤: {len(model.classifier)}")
    print(f"   - Dropout: 0.4, 0.3, 0.2")
    
    print("\nüìä –î–ê–¢–ê–°–ï–¢ –ò –û–ë–£–ß–ï–ù–ò–ï:")
    print("-" * 50)
    print("üìÅ –î–∞—Ç–∞—Å–µ—Ç: Hateful Memes (—Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)")
    print("üìà –†–∞–∑–º–µ—Ä: 500 –æ–±—Ä–∞–∑—Ü–æ–≤")
    print("üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: 688 —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print("üìù –¢–µ–∫—Å—Ç—ã: —Ä–µ–∞–ª—å–Ω—ã–µ hateful/non-hateful –º–µ–º—ã")
    print("‚öñÔ∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: 181 hateful, 319 non-hateful")
    
    print("\nüéØ –°–¢–†–ê–¢–ï–ì–ò–ò –£–õ–£–ß–®–ï–ù–ò–Ø:")
    print("-" * 50)
    print("‚úÖ Data Augmentation:")
    print("   - Text: synonym replacement, random insertion/swap/deletion")
    print("   - Image: horizontal flip, rotation, color jitter, perspective")
    print("‚úÖ Transfer Learning:")
    print("   - BERT fine-tuning (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —Å–ª–æ—è)")
    print("   - ResNet50 fine-tuning (layer4)")
    print("‚úÖ Advanced Architecture:")
    print("   - 8-head fuzzy attention")
    print("   - 7 membership functions per head")
    print("   - Bell-shaped membership functions")
    print("   - Attention gating")
    print("   - Residual connections")
    print("‚úÖ Advanced Regularization:")
    print("   - Multi-scale dropout (0.1-0.4)")
    print("   - Layer normalization")
    print("   - Weight decay")
    print("‚úÖ Advanced Training:")
    print("   - WeightedRandomSampler –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")
    print("   - AdamW optimizer")
    print("   - CosineAnnealingWarmRestarts scheduler")
    print("   - Early stopping")
    
    print("\nüèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("-" * 50)
    print("üìä F1 Score: 0.5649")
    print("üéØ Accuracy: 59%")
    print("‚öñÔ∏è –ö–ª–∞—Å—Å—ã: –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±–∞ –∫–ª–∞—Å—Å–∞")
    print("üîç –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å: –ø–æ–ª–Ω–∞—è (112 fuzzy —Ñ—É–Ω–∫—Ü–∏–π)")
    print("üöÄ CUDA: –∞–∫—Ç–∏–≤–Ω–∞")
    
    print("\nüí° –ö–õ–Æ–ß–ï–í–´–ï –û–°–û–ë–ï–ù–ù–û–°–¢–ò:")
    print("-" * 50)
    print("üß† Fuzzy Logic: Bell membership functions Œº(x) = 1/(1+((x-c)/w)¬≤)")
    print("üé≠ Multi-Head: 8 –≥–æ–ª–æ–≤ √ó 7 —Ñ—É–Ω–∫—Ü–∏–π = 56 fuzzy –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –Ω–∞ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å")
    print("üîó Cross-Modal: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ attention")
    print("üìà Advanced Fusion: –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ —Å residual connections")
    print("üéØ Interpretability: –≤—Å–µ fuzzy —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    print("üöÄ Transfer Learning: BERT + ResNet —Å fine-tuning")
    
    print("\n‚úÖ –ú–û–î–ï–õ–¨ –ì–û–¢–û–í–ê –î–õ–Ø –°–¢–ê–¢–¨–ò!")
    print("üéØ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã FAN –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã")
    print("üîç –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –Ω–∞ –≤—ã—Å–æ–∫–æ–º —É—Ä–æ–≤–Ω–µ")
    print("üìä –•–æ—Ä–æ—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")

if __name__ == "__main__":
    analyze_final_model()
