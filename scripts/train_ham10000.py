#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è FAN –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ HAM10000
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import json
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from dataset_manager import DatasetManager
from advanced_fan_model import AdvancedFANModel

class Trainer:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è FAN –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, dataset_name, batch_size=8, learning_rate=1e-4):
        self.dataset_name = dataset_name
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.dataset_manager = DatasetManager()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã
        self.train_dataloader = self.dataset_manager.create_dataloader(
            dataset_name, split='train', batch_size=batch_size, use_balanced_sampling=True
        )
        self.val_dataloader = self.dataset_manager.create_dataloader(
            dataset_name, split='val', batch_size=batch_size, use_balanced_sampling=False
        )
        
        self.train_dataset = self.dataset_manager.create_dataset(dataset_name, split='train')
        self.val_dataset = self.dataset_manager.create_dataset(dataset_name, split='val')
        
        # –°–æ–∑–¥–∞–µ–º —É—Å–ª–æ–∂–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        self.model = AdvancedFANModel(
            num_classes=self.train_dataset.num_classes,
            num_heads=8,  # 8 –≥–æ–ª–æ–≤ (512 –¥–µ–ª–∏—Ç—Å—è –Ω–∞ 8)
            hidden_dim=512,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è HAM10000
            use_bert=True,
            use_resnet=True
        ).to(self.device)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ loss
        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=15, eta_min=1e-6)
        self.criterion = nn.CrossEntropyLoss()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.train_losses = []
        self.val_losses = []
        self.f1_scores = []
        self.accuracies = []
        
        print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è {dataset_name}")
        print(f"üìä –ö–ª–∞—Å—Å–æ–≤: {self.train_dataset.num_classes}")
        print(f"üìä Train samples: {len(self.train_dataset)}")
        print(f"üìä Val samples: {len(self.val_dataset)}")
        print(f"üñ•Ô∏è Device: {self.device}")

    def train_epoch(self):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        total_loss = 0
        all_predictions = []
        all_labels = []
        
        progress_bar = tqdm(self.train_dataloader, desc="Training")
        
        for batch_idx, batch in enumerate(progress_bar):
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            text_tokens = batch['text_tokens'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            image_features = batch['image'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            self.optimizer.zero_grad()
            
            # Forward pass
            model_output = self.model(text_tokens, attention_mask, image_features)
            if isinstance(model_output, dict):
                outputs = model_output['logits']
            else:
                outputs = model_output
            loss = self.criterion(outputs, labels)
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            total_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # –û–±–Ω–æ–≤–ª—è–µ–º progress bar
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'
            })
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        avg_loss = total_loss / len(self.train_dataloader)
        f1 = f1_score(all_labels, all_predictions, average='weighted')
        accuracy = accuracy_score(all_labels, all_predictions)
        
        return avg_loss, f1, accuracy

    def validate_epoch(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_dataloader, desc="Validation"):
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                text_tokens = batch['text_tokens'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                image_features = batch['image'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # Forward pass
                model_output = self.model(text_tokens, attention_mask, image_features)
                if isinstance(model_output, dict):
                    outputs = model_output['logits']
                else:
                    outputs = model_output
                loss = self.criterion(outputs, labels)
                
                # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                total_loss += loss.item()
                predictions = torch.argmax(outputs, dim=1)
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        avg_loss = total_loss / len(self.val_dataloader)
        f1 = f1_score(all_labels, all_predictions, average='weighted')
        accuracy = accuracy_score(all_labels, all_predictions)
        precision = precision_score(all_labels, all_predictions, average='weighted')
        recall = recall_score(all_labels, all_predictions, average='weighted')
        
        return avg_loss, f1, accuracy, precision, recall

    def train(self, num_epochs=15):
        """–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {num_epochs} —ç–ø–æ—Ö")
        print("=" * 60)
        
        best_f1 = 0
        best_accuracy = 0
        patience = 5
        patience_counter = 0
        
        for epoch in range(num_epochs):
            print(f"\nüìÖ –≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}")
            print("-" * 40)
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_f1, train_acc = self.train_epoch()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_f1, val_acc, val_precision, val_recall = self.validate_epoch()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º scheduler
            self.scheduler.step()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.f1_scores.append(val_f1)
            self.accuracies.append(val_acc)
            
            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print(f"üìä Train Loss: {train_loss:.4f}, F1: {train_f1:.4f}, Acc: {train_acc:.4f}")
            print(f"üìä Val Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Acc: {val_acc:.4f}")
            print(f"üìä Val Precision: {val_precision:.4f}, Recall: {val_recall:.4f}")
            print(f"üìä Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if val_f1 > best_f1:
                best_f1 = val_f1
                best_accuracy = val_acc
                patience_counter = 0
                self.save_model(f"best_{self.dataset_name}_fan_model.pth")
                print(f"‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! F1: {best_f1:.4f}, Acc: {best_accuracy:.4f}")
            else:
                patience_counter += 1
                print(f"‚è≥ Patience: {patience_counter}/{patience}")
            
            # Early stopping
            if patience_counter >= patience:
                print(f"üõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                break
        
        print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üèÜ –õ—É—á—à–∏–π F1 Score: {best_f1:.4f}")
        print(f"üèÜ –õ—É—á—à–∞—è Accuracy: {best_accuracy:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        self.save_plots()
        
        return best_f1, best_accuracy

    def save_model(self, filename):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å"""
        model_dir = Path(f"models/{self.dataset_name}")
        model_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = model_dir / filename
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'num_classes': self.train_dataset.num_classes,
            'class_names': self.train_dataset.class_names,
            'dataset_name': self.dataset_name
        }, model_path)
        
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

    def save_plots(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        model_dir = Path(f"models/{self.dataset_name}")
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # –ì—Ä–∞—Ñ–∏–∫ loss
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(self.train_losses, label='Train Loss', color='blue')
        plt.plot(self.val_losses, label='Val Loss', color='red')
        plt.title('Training and Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        plt.plot(self.f1_scores, label='F1 Score', color='green')
        plt.plot(self.accuracies, label='Accuracy', color='orange')
        plt.title('F1 Score and Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Score')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig(model_dir / f"{self.dataset_name}_training_metrics.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {model_dir / f'{self.dataset_name}_training_metrics.png'}")

def main():
    print("üè• –û–±—É—á–µ–Ω–∏–µ FAN –º–æ–¥–µ–ª–∏ –Ω–∞ HAM10000")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name()}")
    else:
        print("‚ö†Ô∏è CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = Trainer(
        dataset_name='ham10000',
        batch_size=8,
        learning_rate=1e-4
    )
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    best_f1, best_accuracy = trainer.train(num_epochs=15)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–∏
    target_f1 = 0.90
    target_accuracy = 0.90
    
    print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"üìä F1 Score: {best_f1:.4f} (—Ü–µ–ª—å: {target_f1:.2f})")
    print(f"üìä Accuracy: {best_accuracy:.4f} (—Ü–µ–ª—å: {target_accuracy:.2f})")
    
    if best_f1 >= target_f1 and best_accuracy >= target_accuracy:
        print("üéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ —É—Ä–æ–≤–Ω—è A!")
    else:
        print("‚ö†Ô∏è –¶–µ–ª—å –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞, –Ω–æ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    
    print(f"\nüìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: models/ham10000/")

if __name__ == "__main__":
    main()
