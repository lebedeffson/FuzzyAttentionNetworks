#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ FAN –º–æ–¥–µ–ª–∏ –Ω–∞ Stanford Dogs –¥–∞—Ç–∞—Å–µ—Ç–µ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from pathlib import Path
import json
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from dataset_manager import DatasetManager
from universal_fan_model import UniversalFANModel

class StanfordDogsTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è Stanford Dogs FAN –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, data_dir, model_dir, device='cuda'):
        self.data_dir = Path(data_dir)
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        self.dataset_manager = DatasetManager()
        self.train_dataset = self.dataset_manager.create_dataset('stanford_dogs', 'train')
        self.val_dataset = self.dataset_manager.create_dataset('stanford_dogs', 'val')
        
        print(f"üìä Train samples: {len(self.train_dataset)}")
        print(f"üìä Val samples: {len(self.val_dataset)}")
        print(f"üìä Classes: {self.train_dataset.num_classes}")
        
        # –°–æ–∑–¥–∞–µ–º DataLoader'—ã
        self.train_loader = DataLoader(
            self.train_dataset, 
            batch_size=16, 
            shuffle=True, 
            num_workers=2,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        self.val_loader = DataLoader(
            self.val_dataset, 
            batch_size=16, 
            shuffle=False, 
            num_workers=2,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = UniversalFANModel(
            num_classes=self.train_dataset.num_classes,
            num_heads=8,  # –ë–æ–ª—å—à–µ –≥–æ–ª–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏
            hidden_dim=768,  # –ë–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            use_bert=True,
            use_resnet=True
        ).to(self.device)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)
        self.criterion = nn.CrossEntropyLoss()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.train_losses = []
        self.val_losses = []
        self.val_accuracies = []
        self.val_f1_scores = []
        
        self.best_f1 = 0.0
        self.best_accuracy = 0.0
        
    def train_epoch(self, epoch):
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')
        
        for batch_idx, batch in enumerate(pbar):
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            text_tokens = batch['text_tokens'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            images = batch['image'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            self.optimizer.zero_grad()
            
            # Forward pass
            outputs = self.model(text_tokens, attention_mask, images)
            logits = outputs['logits']
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
            loss = self.criterion(logits, labels)
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
            self.optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_loss += loss.item()
            _, predicted = torch.max(logits.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        self.train_losses.append(avg_loss)
        
        return avg_loss, accuracy
    
    def validate_epoch(self, epoch):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc=f'Validation {epoch+1}'):
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                text_tokens = batch['text_tokens'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                images = batch['image'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # Forward pass
                outputs = self.model(text_tokens, attention_mask, images)
                logits = outputs['logits']
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
                loss = self.criterion(logits, labels)
                total_loss += loss.item()
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                _, predicted = torch.max(logits.data, 1)
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        avg_loss = total_loss / len(self.val_loader)
        accuracy = accuracy_score(all_labels, all_predictions)
        f1 = f1_score(all_labels, all_predictions, average='weighted')
        precision = precision_score(all_labels, all_predictions, average='weighted')
        recall = recall_score(all_labels, all_predictions, average='weighted')
        
        self.val_losses.append(avg_loss)
        self.val_accuracies.append(accuracy)
        self.val_f1_scores.append(f1)
        
        return avg_loss, accuracy, f1, precision, recall
    
    def save_model(self, epoch, accuracy, f1_score, is_best=False):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'accuracy': accuracy,
            'f1_score': f1_score,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'val_f1_scores': self.val_f1_scores
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint
        checkpoint_path = self.model_dir / f'checkpoint_epoch_{epoch+1}.pth'
        torch.save(checkpoint, checkpoint_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if is_best:
            best_path = self.model_dir / 'best_stanford_dogs_fan_model.pth'
            torch.save(checkpoint, best_path)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_path}")
    
    def plot_metrics(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –º–µ—Ç—Ä–∏–∫"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        ax1.plot(self.train_losses, label='Train Loss', color='blue')
        ax1.plot(self.val_losses, label='Val Loss', color='red')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # Accuracy
        ax2.plot(self.val_accuracies, label='Val Accuracy', color='green')
        ax2.set_title('Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)
        
        # F1 Score
        ax3.plot(self.val_f1_scores, label='Val F1 Score', color='orange')
        ax3.set_title('Validation F1 Score')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('F1 Score')
        ax3.legend()
        ax3.grid(True)
        
        # Learning Rate
        lrs = [self.scheduler.get_last_lr()[0] for _ in range(len(self.train_losses))]
        ax4.plot(lrs, label='Learning Rate', color='purple')
        ax4.set_title('Learning Rate Schedule')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Learning Rate')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        plt.savefig(self.model_dir / 'training_metrics.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def train(self, num_epochs=50, patience=10):
        """–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {num_epochs} —ç–ø–æ—Ö")
        print("=" * 50)
        
        start_time = time.time()
        best_epoch = 0
        patience_counter = 0
        
        for epoch in range(num_epochs):
            print(f"\nüìà –≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}")
            print("-" * 30)
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc, val_f1, val_prec, val_rec = self.validate_epoch(epoch)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
            self.scheduler.step()
            
            # –í—ã–≤–æ–¥–∏–º –º–µ—Ç—Ä–∏–∫–∏
            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            print(f"Val F1: {val_f1:.4f}, Val Prec: {val_prec:.4f}, Val Rec: {val_rec:.4f}")
            print(f"Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            is_best = False
            if val_f1 > self.best_f1:
                self.best_f1 = val_f1
                self.best_accuracy = val_acc
                best_epoch = epoch
                is_best = True
                patience_counter = 0
                print(f"üéâ –ù–æ–≤—ã–π –ª—É—á—à–∏–π F1: {val_f1:.4f}")
            else:
                patience_counter += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.save_model(epoch, val_acc, val_f1, is_best)
            
            # Early stopping
            if patience_counter >= patience:
                print(f"‚èπÔ∏è Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–∏
            if val_f1 >= 0.75:
                print(f"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª—å F1 >= 0.75: {val_f1:.4f}")
                break
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = time.time() - start_time
        print(f"\nüèÅ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
        print(f"üìä –õ—É—á—à–∏–π F1: {self.best_f1:.4f}")
        print(f"üìä –õ—É—á—à–∞—è Accuracy: {self.best_accuracy:.4f}")
        print(f"üìä –õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {best_epoch+1}")
        
        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏
        self.plot_metrics()
        
        return self.best_f1, self.best_accuracy

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üêï –û–±—É—á–µ–Ω–∏–µ FAN –º–æ–¥–µ–ª–∏ –Ω–∞ Stanford Dogs")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    data_dir = Path("data/stanford_dogs_fan")
    if not data_dir.exists():
        print("‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ download_stanford_dogs.py")
        return
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = StanfordDogsTrainer(
        data_dir=data_dir,
        model_dir="models/stanford_dogs",
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    best_f1, best_acc = trainer.train(num_epochs=50, patience=10)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç
    if best_f1 >= 0.75:
        print(f"\nüéâ –£–°–ü–ï–•! –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª—å F1 >= 0.75: {best_f1:.4f}")
    else:
        print(f"\n‚ö†Ô∏è –¶–µ–ª—å –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞. F1: {best_f1:.4f}")
        print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É")

if __name__ == "__main__":
    main()

