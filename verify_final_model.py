#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
"""

import torch
import torch.nn as nn
import json
import numpy as np
from pathlib import Path
from PIL import Image
import sys
import os
import math
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from collections import Counter
import torchvision.transforms as transforms
from transformers import BertTokenizer, BertModel
import torchvision.models as models

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

class AdvancedFuzzyAttention(nn.Module):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è fuzzy attention —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self, hidden_dim, num_heads=8, num_membership=7):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        self.num_membership = num_membership
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ fuzzy membership functions
        self.fuzzy_centers = nn.Parameter(torch.randn(num_heads, num_membership, self.head_dim) * 0.05)
        self.fuzzy_widths = nn.Parameter(torch.ones(num_heads, num_membership, self.head_dim) * 0.2)
        
        # Multi-scale attention
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, hidden_dim)
        
        # Residual connection
        self.norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.1)
        
        # Attention gating
        self.attention_gate = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()
        )
        
    def bell_membership(self, x, center, width):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–ª–æ–∫–æ–ª–æ–æ–±—Ä–∞–∑–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏"""
        return 1 / (1 + ((x - center) / (width + 1e-8)) ** 2)
        
    def forward(self, query, key, value, return_interpretation=False):
        batch_size = query.size(0)
        residual = query
        
        # Linear projections
        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # Apply fuzzy membership functions
        fuzzy_scores = torch.zeros_like(scores)
        membership_values = {}
        
        for h in range(self.num_heads):
            for f in range(self.num_membership):
                center = self.fuzzy_centers[h, f].unsqueeze(0).unsqueeze(0)
                width = self.fuzzy_widths[h, f].unsqueeze(0).unsqueeze(0)
                
                # Bell membership function
                membership = self.bell_membership(scores[:, h], center, width)
                fuzzy_scores[:, h] += membership.mean(dim=-1, keepdim=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
                if return_interpretation:
                    membership_values[f'head_{h}_func_{f}'] = {
                        'center': center,
                        'width': width,
                        'membership': membership,
                        'contribution': membership.mean(dim=-1, keepdim=True)
                    }
        
        # Normalize
        fuzzy_scores = fuzzy_scores / self.num_membership
        
        # Apply softmax
        attention_weights = torch.softmax(fuzzy_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        attended = torch.matmul(attention_weights, V)
        
        # Concatenate heads
        attended = attended.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)
        
        # Output projection
        output = self.output(attended)
        
        # Attention gating
        gate = self.attention_gate(output)
        output = output * gate
        
        # Residual connection
        output = self.norm(output + residual)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        if return_interpretation:
            self.attention_weights = attention_weights
            self.fuzzy_scores = fuzzy_scores
            self.membership_values = membership_values
        
        return output, attention_weights

class AdvancedFANModel(nn.Module):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è FAN –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
    
    def __init__(self, num_classes=2, num_heads=8, hidden_dim=768):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # BERT –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å fine-tuning
        self.bert_model = BertModel.from_pretrained('bert-base-uncased')
        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ BERT
        for param in self.bert_model.encoder.layer[-2:].parameters():
            param.requires_grad = True
        
        # ResNet –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å fine-tuning
        self.resnet = models.resnet50(pretrained=True)
        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ ResNet
        for param in self.resnet.layer4.parameters():
            param.requires_grad = True
        
        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, hidden_dim)
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ Fuzzy Attention Networks
        self.text_fuzzy_attention = AdvancedFuzzyAttention(hidden_dim, num_heads, 7)
        self.image_fuzzy_attention = AdvancedFuzzyAttention(hidden_dim, num_heads, 7)
        
        # Multi-scale cross-modal attention
        self.cross_modal_attention = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True, dropout=0.1)
        
        # Advanced fusion layers
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.LayerNorm(hidden_dim)
        )
        
        # Advanced classification head
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, text_tokens, attention_mask, image, return_explanations=False):
        batch_size = text_tokens.size(0)
        
        # BERT encoding –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        bert_outputs = self.bert_model(text_tokens, attention_mask=attention_mask)
        text_features = bert_outputs.last_hidden_state.mean(dim=1)
        
        # ResNet encoding –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_features = self.resnet(image)
        
        # Fuzzy attention –Ω–∞ —Ç–µ–∫—Å—Ç–µ
        text_attended, text_attention_weights = self.text_fuzzy_attention(
            text_features.unsqueeze(1), text_features.unsqueeze(1), text_features.unsqueeze(1),
            return_interpretation=return_explanations
        )
        text_attended = text_attended.squeeze(1)
        
        # Fuzzy attention –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
        image_attended, image_attention_weights = self.image_fuzzy_attention(
            image_features.unsqueeze(1), image_features.unsqueeze(1), image_features.unsqueeze(1),
            return_interpretation=return_explanations
        )
        image_attended = image_attended.squeeze(1)
        
        # Cross-modal attention
        text_enhanced, cross_modal_weights = self.cross_modal_attention(
            text_attended.unsqueeze(1), image_attended.unsqueeze(1), image_attended.unsqueeze(1)
        )
        text_enhanced = text_enhanced.squeeze(1)
        
        # Fusion
        combined = torch.cat([text_enhanced, image_attended], dim=1)
        fused = self.fusion_layer(combined)
        
        # Classification
        logits = self.classifier(fused)
        probs = torch.softmax(logits, dim=1)
        
        result = {
            'logits': logits,
            'probs': probs,
            'predictions': torch.argmax(logits, dim=1),
            'confidence': torch.max(probs, dim=1)[0]
        }
        
        if return_explanations:
            result['explanations'] = {
                'text_attention': text_attention_weights,
                'image_attention': image_attention_weights,
                'cross_modal_attention': cross_modal_weights,
                'text_fuzzy_membership': self.text_fuzzy_attention.membership_values,
                'image_fuzzy_membership': self.image_fuzzy_attention.membership_values,
                'text_features': text_features,
                'image_features': image_features
            }
            
        return result

def verify_final_model():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...")
    
    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    data_path = Path("data/hateful_memes")
    train_file = data_path / "train.jsonl"
    img_dir = data_path / "img"
    
    print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç: {data_path}")
    print(f"üìÑ –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö: {train_file}")
    print(f"üñºÔ∏è –ü–∞–ø–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {img_dir}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    with open(train_file, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    
    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(data)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    labels = [item['label'] for item in data]
    class_counts = Counter(labels)
    print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {dict(class_counts)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    image_files = list(img_dir.glob("*.png"))
    print(f"üñºÔ∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_files)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:")
    for i in range(min(5, len(data))):
        item = data[i]
        img_path = img_dir / item['img'].replace('img/', '')
        if img_path.exists():
            try:
                img = Image.open(img_path)
                print(f"  ‚úÖ {item['img']}: {img.size}, {img.mode}")
            except Exception as e:
                print(f"  ‚ùå {item['img']}: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ - {e}")
        else:
            print(f"  ‚ùå {item['img']}: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model_path = "models/best_advanced_metrics_model.pth"
    if os.path.exists(model_path):
        print(f"\nü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_path}")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = AdvancedFANModel(
            num_classes=2,
            num_heads=8,
            hidden_dim=768
        ).to(device)
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            model.load_state_dict(torch.load(model_path, map_location=device))
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
            print(f"\nüèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
            print(f"  - BERT: {model.bert_model.__class__.__name__}")
            print(f"  - ResNet: {model.resnet.__class__.__name__}")
            print(f"  - Text Fuzzy Attention: {model.text_fuzzy_attention.__class__.__name__}")
            print(f"  - Image Fuzzy Attention: {model.image_fuzzy_attention.__class__.__name__}")
            print(f"  - Cross-modal Attention: {model.cross_modal_attention.__class__.__name__}")
            print(f"  - Classifier: {len(model.classifier)} —Å–ª–æ–µ–≤")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–∞—Ö
            print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            
            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
            transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for i in range(min(10, len(data))):
                    item = data[i]
                    
                    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
                    encoding = tokenizer(
                        item['text'],
                        truncation=True,
                        padding='max_length',
                        max_length=128,
                        return_tensors='pt'
                    )
                    
                    text_tokens = encoding['input_ids'].to(device)
                    attention_mask = encoding['attention_mask'].to(device)
                    
                    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    img_path = img_dir / item['img'].replace('img/', '')
                    if img_path.exists():
                        try:
                            img = Image.open(img_path).convert('RGB')
                            image_tensor = transform(img).unsqueeze(0).to(device)
                            
                            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                            result = model(text_tokens, attention_mask, image_tensor, return_explanations=True)
                            
                            prediction = result['predictions'][0].item()
                            confidence = result['confidence'][0].item()
                            true_label = item['label']
                            
                            is_correct = prediction == true_label
                            if is_correct:
                                correct += 1
                            total += 1
                            
                            print(f"  –û–±—Ä–∞–∑–µ—Ü {i+1}:")
                            print(f"    –¢–µ–∫—Å—Ç: {item['text'][:50]}...")
                            print(f"    –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {item['img']}")
                            print(f"    –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {true_label}")
                            print(f"    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {prediction}")
                            print(f"    –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}")
                            print(f"    –†–µ–∑—É–ª—å—Ç–∞—Ç: {'‚úÖ' if is_correct else '‚ùå'}")
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
                            if 'explanations' in result:
                                explanations = result['explanations']
                                print(f"    –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å:")
                                print(f"      - Text attention weights: {explanations['text_attention'].shape}")
                                print(f"      - Image attention weights: {explanations['image_attention'].shape}")
                                print(f"      - Cross-modal attention: {explanations['cross_modal_attention'].shape}")
                                print(f"      - Text fuzzy membership: {len(explanations['text_fuzzy_membership'])} —Ñ—É–Ω–∫—Ü–∏–π")
                                print(f"      - Image fuzzy membership: {len(explanations['image_fuzzy_membership'])} —Ñ—É–Ω–∫—Ü–∏–π")
                            
                            print()
                            
                        except Exception as e:
                            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–±—Ä–∞–∑—Ü–∞ {i+1}: {e}")
                    else:
                        print(f"  ‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {item['img']}")
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
            if total > 0:
                accuracy = correct / total
                print(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ {total} –æ–±—Ä–∞–∑—Ü–∞—Ö: {accuracy:.3f}")
            
            print(f"\n‚úÖ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
            print(f"üéØ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")
            print(f"üß† –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã FAN –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∞–∫—Ç–∏–≤–Ω—ã")
            print(f"üîç –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞")
            print(f"üöÄ Transfer Learning (BERT + ResNet) –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
    else:
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")

if __name__ == "__main__":
    verify_final_model()
