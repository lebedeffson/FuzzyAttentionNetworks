"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª –¥–ª—è Fuzzy Attention Networks
–°–æ–∑–¥–∞–µ—Ç –±–æ–ª–µ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –ø—Ä–∞–≤–∏–ª–∞
"""

import torch
import torch.nn as nn
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
import json
import re

@dataclass
class SemanticFuzzyRule:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ fuzzy –ø—Ä–∞–≤–∏–ª–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    rule_id: str
    condition_text: str
    condition_image: str
    conclusion: str
    confidence: float
    strength: float
    semantic_type: str  # 'color', 'texture', 'shape', 'object', 'spatial'
    linguistic_description: str
    membership_values: Dict[str, float]
    attention_head: int
    tnorm_type: str

class ImprovedRuleExtractor:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –ø—Ä–∞–≤–∏–ª —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    
    def __init__(self, 
                 attention_threshold: float = 0.1,
                 strong_threshold: float = 0.15,
                 max_rules_per_head: int = 5):
        self.attention_threshold = attention_threshold
        self.strong_threshold = strong_threshold
        self.max_rules_per_head = max_rules_per_head
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.semantic_templates = {
            'color': {
                'patterns': ['red', 'blue', 'green', 'yellow', 'black', 'white', 'brown', 'gray'],
                'descriptions': ['—Ü–≤–µ—Ç', '–æ–∫—Ä–∞—Å–∫–∞', '–æ—Ç—Ç–µ–Ω–æ–∫', '—Ç–æ–Ω']
            },
            'texture': {
                'patterns': ['smooth', 'rough', 'soft', 'hard', 'shiny', 'matte', 'fuzzy', 'glossy'],
                'descriptions': ['—Ç–µ–∫—Å—Ç—É—Ä–∞', '–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å', '–º–∞—Ç–µ—Ä–∏–∞–ª', '—Å—Ç—Ä—É–∫—Ç—É—Ä–∞']
            },
            'shape': {
                'patterns': ['round', 'square', 'triangular', 'oval', 'rectangular', 'circular', 'angular'],
                'descriptions': ['—Ñ–æ—Ä–º–∞', '–æ—á–µ—Ä—Ç–∞–Ω–∏—è', '–∫–æ–Ω—Ç—É—Ä', '–≥–µ–æ–º–µ—Ç—Ä–∏—è']
            },
            'object': {
                'patterns': ['car', 'dog', 'cat', 'house', 'tree', 'person', 'bird', 'fish'],
                'descriptions': ['–æ–±—ä–µ–∫—Ç', '–ø—Ä–µ–¥–º–µ—Ç', '—ç–ª–µ–º–µ–Ω—Ç', '–¥–µ—Ç–∞–ª—å']
            },
            'spatial': {
                'patterns': ['left', 'right', 'top', 'bottom', 'center', 'corner', 'edge', 'middle'],
                'descriptions': ['–ø–æ–∑–∏—Ü–∏—è', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '–ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ']
            }
        }
        
        # –õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è –ø—Ä–∞–≤–∏–ª
        self.rule_templates = {
            'high_confidence': [
                "–ï–°–õ–ò {text_condition} –ò {image_condition} –¢–û {conclusion} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})",
                "–ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ {text_condition} –∏ {image_condition} –º–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç {conclusion} —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {confidence:.1%}",
                "–ü—Ä–∞–≤–∏–ª–æ: {text_condition} + {image_condition} ‚Üí {conclusion} ({confidence:.1%})"
            ],
            'medium_confidence': [
                "–ï–°–õ–ò {text_condition} –ò {image_condition} –¢–û –≤–µ—Ä–æ—è—Ç–Ω–æ {conclusion} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})",
                "–ü—Ä–∏ {text_condition} –∏ {image_condition} –º–æ–¥–µ–ª—å —Å–∫–ª–æ–Ω—è–µ—Ç—Å—è –∫ {conclusion} ({confidence:.1%})",
                "–£—Å–ª–æ–≤–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ: {text_condition} + {image_condition} ‚Üí {conclusion} ({confidence:.1%})"
            ],
            'low_confidence': [
                "–ï–°–õ–ò {text_condition} –ò {image_condition} –¢–û –≤–æ–∑–º–æ–∂–Ω–æ {conclusion} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})",
                "–ü—Ä–∏ {text_condition} –∏ {image_condition} –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ {conclusion} ({confidence:.1%})",
                "–°–ª–∞–±–æ–µ –ø—Ä–∞–≤–∏–ª–æ: {text_condition} + {image_condition} ‚Üí {conclusion} ({confidence:.1%})"
            ]
        }
    
    def extract_semantic_rules(self, 
                             attention_weights: torch.Tensor,
                             text_tokens: Optional[List[str]] = None,
                             image_features: Optional[torch.Tensor] = None,
                             class_names: Optional[List[str]] = None,
                             head_idx: int = 0) -> List[SemanticFuzzyRule]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ attention weights
        """
        rules = []
        batch_size, seq_len, _ = attention_weights.shape
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞
        for batch_idx in range(batch_size):
            attention = attention_weights[batch_idx]
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–∏–ª—å–Ω—ã–µ —Å–≤—è–∑–∏
            high_attention_mask = attention > self.attention_threshold
            high_attention_indices = torch.nonzero(high_attention_mask, as_tuple=False)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º —Å–≤—è–∑–µ–π
            text_to_image = []
            image_to_text = []
            text_to_text = []
            image_to_image = []
            
            for idx in high_attention_indices:
                from_pos, to_pos = idx[0].item(), idx[1].item()
                strength = attention[from_pos, to_pos].item()
                
                if from_pos == to_pos:
                    continue
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–≤—è–∑–∏ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞ - —Ç–µ–∫—Å—Ç, –≤—Ç–æ—Ä–∞—è - –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
                mid_point = seq_len // 2
                
                if from_pos < mid_point and to_pos >= mid_point:
                    text_to_image.append((from_pos, to_pos, strength))
                elif from_pos >= mid_point and to_pos < mid_point:
                    image_to_text.append((from_pos, to_pos, strength))
                elif from_pos < mid_point and to_pos < mid_point:
                    text_to_text.append((from_pos, to_pos, strength))
                else:
                    image_to_image.append((from_pos, to_pos, strength))
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Å–≤—è–∑–µ–π
            rules.extend(self._extract_text_to_image_rules(text_to_image, text_tokens, class_names, head_idx))
            rules.extend(self._extract_image_to_text_rules(image_to_text, text_tokens, class_names, head_idx))
            rules.extend(self._extract_text_to_text_rules(text_to_text, text_tokens, head_idx))
            rules.extend(self._extract_image_to_image_rules(image_to_image, head_idx))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª–∞
        rules = self._filter_and_rank_semantic_rules(rules)
        return rules[:self.max_rules_per_head]
    
    def _extract_text_to_image_rules(self, 
                                   connections: List[Tuple[int, int, float]],
                                   text_tokens: Optional[List[str]],
                                   class_names: Optional[List[str]],
                                   head_idx: int) -> List[SemanticFuzzyRule]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
        rules = []
        
        for from_pos, to_pos, strength in connections:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ç–∏–ø –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞
            semantic_type = self._classify_text_semantic_type(text_tokens, from_pos)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É—Å–ª–æ–≤–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            text_condition = self._generate_text_condition(text_tokens, from_pos, semantic_type)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É—Å–ª–æ–≤–∏–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_condition = self._generate_image_condition(to_pos, semantic_type)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–∫–ª—é—á–µ–Ω–∏–µ
            conclusion = self._generate_conclusion(class_names, strength)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = min(strength / self.strong_threshold, 1.0)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            linguistic_desc = self._generate_linguistic_rule(
                text_condition, image_condition, conclusion, confidence
            )
            
            rule = SemanticFuzzyRule(
                rule_id=f"rule_{head_idx}_{from_pos}_{to_pos}",
                condition_text=text_condition,
                condition_image=image_condition,
                conclusion=conclusion,
                confidence=confidence,
                strength=strength,
                semantic_type=semantic_type,
                linguistic_description=linguistic_desc,
                membership_values={'text': strength, 'image': strength * 0.8},
                attention_head=head_idx,
                tnorm_type='product'
            )
            
            rules.append(rule)
        
        return rules
    
    def _extract_image_to_text_rules(self, 
                                   connections: List[Tuple[int, int, float]],
                                   text_tokens: Optional[List[str]],
                                   class_names: Optional[List[str]],
                                   head_idx: int) -> List[SemanticFuzzyRule]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        rules = []
        
        for from_pos, to_pos, strength in connections:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ç–∏–ø
            semantic_type = self._classify_text_semantic_type(text_tokens, to_pos)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É—Å–ª–æ–≤–∏—è
            image_condition = self._generate_image_condition(from_pos, semantic_type)
            text_condition = self._generate_text_condition(text_tokens, to_pos, semantic_type)
            conclusion = self._generate_conclusion(class_names, strength)
            
            confidence = min(strength / self.strong_threshold, 1.0)
            linguistic_desc = self._generate_linguistic_rule(
                image_condition, text_condition, conclusion, confidence
            )
            
            rule = SemanticFuzzyRule(
                rule_id=f"rule_{head_idx}_{from_pos}_{to_pos}",
                condition_text=text_condition,
                condition_image=image_condition,
                conclusion=conclusion,
                confidence=confidence,
                strength=strength,
                semantic_type=semantic_type,
                linguistic_description=linguistic_desc,
                membership_values={'image': strength, 'text': strength * 0.8},
                attention_head=head_idx,
                tnorm_type='product'
            )
            
            rules.append(rule)
        
        return rules
    
    def _extract_text_to_text_rules(self, 
                                  connections: List[Tuple[int, int, float]],
                                  text_tokens: Optional[List[str]],
                                  head_idx: int) -> List[SemanticFuzzyRule]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
        rules = []
        
        for from_pos, to_pos, strength in connections:
            semantic_type = 'text_relation'
            
            text_condition = f"—Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç '{text_tokens[from_pos] if text_tokens and from_pos < len(text_tokens) else f'–ø—Ä–∏–∑–Ω–∞–∫_{from_pos}'}'"
            related_condition = f"—Å–≤—è–∑–∞–Ω —Å '{text_tokens[to_pos] if text_tokens and to_pos < len(text_tokens) else f'–ø—Ä–∏–∑–Ω–∞–∫_{to_pos}'}'"
            conclusion = "—É—Å–∏–ª–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ"
            
            confidence = min(strength / self.strong_threshold, 1.0)
            linguistic_desc = f"–ï–°–õ–ò {text_condition} –ò {related_condition} –¢–û {conclusion} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})"
            
            rule = SemanticFuzzyRule(
                rule_id=f"rule_{head_idx}_{from_pos}_{to_pos}",
                condition_text=text_condition,
                condition_image=related_condition,
                conclusion=conclusion,
                confidence=confidence,
                strength=strength,
                semantic_type=semantic_type,
                linguistic_description=linguistic_desc,
                membership_values={'text_from': strength, 'text_to': strength * 0.9},
                attention_head=head_idx,
                tnorm_type='product'
            )
            
            rules.append(rule)
        
        return rules
    
    def _extract_image_to_image_rules(self, 
                                    connections: List[Tuple[int, int, float]],
                                    head_idx: int) -> List[SemanticFuzzyRule]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        rules = []
        
        for from_pos, to_pos, strength in connections:
            semantic_type = 'spatial_relation'
            
            image_condition = f"–æ–±–ª–∞—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {from_pos}"
            related_condition = f"—Å–≤—è–∑–∞–Ω–∞ —Å –æ–±–ª–∞—Å—Ç—å—é {to_pos}"
            conclusion = "—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ"
            
            confidence = min(strength / self.strong_threshold, 1.0)
            linguistic_desc = f"–ï–°–õ–ò {image_condition} –ò {related_condition} –¢–û {conclusion} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})"
            
            rule = SemanticFuzzyRule(
                rule_id=f"rule_{head_idx}_{from_pos}_{to_pos}",
                condition_text=image_condition,
                condition_image=related_condition,
                conclusion=conclusion,
                confidence=confidence,
                strength=strength,
                semantic_type=semantic_type,
                linguistic_description=linguistic_desc,
                membership_values={'image_from': strength, 'image_to': strength * 0.9},
                attention_head=head_idx,
                tnorm_type='product'
            )
            
            rules.append(rule)
        
        return rules
    
    def _classify_text_semantic_type(self, text_tokens: Optional[List[str]], pos: int) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ç–∏–ø —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞"""
        if not text_tokens or pos >= len(text_tokens):
            return 'general'
        
        token = text_tokens[pos].lower()
        
        for semantic_type, info in self.semantic_templates.items():
            for pattern in info['patterns']:
                if pattern in token:
                    return semantic_type
        
        return 'general'
    
    def _generate_text_condition(self, text_tokens: Optional[List[str]], pos: int, semantic_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —É—Å–ª–æ–≤–∏–µ"""
        if not text_tokens or pos >= len(text_tokens):
            return f"—Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–∑–Ω–∞–∫ –ø–æ–∑–∏—Ü–∏–∏ {pos}"
        
        token = text_tokens[pos]
        descriptions = self.semantic_templates.get(semantic_type, {}).get('descriptions', ['–ø—Ä–∏–∑–Ω–∞–∫'])
        desc = np.random.choice(descriptions)
        
        return f"—Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç '{token}' ({desc})"
    
    def _generate_image_condition(self, pos: int, semantic_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É—Å–ª–æ–≤–∏–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        descriptions = self.semantic_templates.get(semantic_type, {}).get('descriptions', ['–ø—Ä–∏–∑–Ω–∞–∫'])
        desc = np.random.choice(descriptions)
        
        return f"–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–º–µ–µ—Ç {desc} –≤ –æ–±–ª–∞—Å—Ç–∏ {pos}"
    
    def _generate_conclusion(self, class_names: Optional[List[str]], strength: float) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞"""
        if class_names and strength > 0.5:
            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            class_name = np.random.choice(class_names)
            return f"–∫–ª–∞—Å—Å '{class_name}'"
        elif strength > 0.3:
            return "–≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"
        else:
            return "—Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"
    
    def _generate_linguistic_rule(self, condition1: str, condition2: str, conclusion: str, confidence: float) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞"""
        if confidence > 0.7:
            template = np.random.choice(self.rule_templates['high_confidence'])
        elif confidence > 0.4:
            template = np.random.choice(self.rule_templates['medium_confidence'])
        else:
            template = np.random.choice(self.rule_templates['low_confidence'])
        
        return template.format(
            text_condition=condition1,
            image_condition=condition2,
            conclusion=conclusion,
            confidence=confidence
        )
    
    def _filter_and_rank_semantic_rules(self, rules: List[SemanticFuzzyRule]) -> List[SemanticFuzzyRule]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∏–ª–µ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        rules.sort(key=lambda r: (r.strength * r.confidence), reverse=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–∏–ø—É –∏ —É—Å–ª–æ–≤–∏—è–º
        unique_rules = []
        seen_conditions = set()
        
        for rule in rules:
            condition_key = (rule.condition_text, rule.condition_image, rule.semantic_type)
            if condition_key not in seen_conditions:
                unique_rules.append(rule)
                seen_conditions.add(condition_key)
        
        return unique_rules
    
    def generate_rule_summary(self, rules: List[SemanticFuzzyRule]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º"""
        if not rules:
            return {"total_rules": 0, "summary": "–ü—Ä–∞–≤–∏–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        type_counts = {}
        confidence_stats = []
        strength_stats = []
        
        for rule in rules:
            rule_type = rule.semantic_type
            type_counts[rule_type] = type_counts.get(rule_type, 0) + 1
            confidence_stats.append(rule.confidence)
            strength_stats.append(rule.strength)
        
        return {
            "total_rules": len(rules),
            "rule_types": type_counts,
            "avg_confidence": np.mean(confidence_stats),
            "avg_strength": np.mean(strength_stats),
            "max_confidence": np.max(confidence_stats),
            "min_confidence": np.min(confidence_stats),
            "summary": f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(rules)} –ø—Ä–∞–≤–∏–ª —Å —Å—Ä–µ–¥–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {np.mean(confidence_stats):.1%}"
        }

def demo_improved_rule_extraction():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª"""
    print("üîç –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª - –î–µ–º–æ")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä attention weights
    seq_len = 10
    attention_weights = torch.rand(1, seq_len, seq_len)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–ª—å–Ω—ã–µ —Å–≤—è–∑–∏
    attention_weights[0, 0, 5] = 0.25  # text to image
    attention_weights[0, 1, 6] = 0.18  # text to image
    attention_weights[0, 5, 1] = 0.20  # image to text
    attention_weights[0, 0, 1] = 0.15  # text to text
    attention_weights[0, 6, 7] = 0.12  # image to image
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    attention_weights = torch.softmax(attention_weights, dim=-1)
    
    # –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    text_tokens = ["red", "car", "smooth", "surface", "round", "wheel", "shiny", "metal", "black", "tire"]
    class_names = ["–∞–≤—Ç–æ–º–æ–±–∏–ª—å", "–≥—Ä—É–∑–æ–≤–∏–∫", "–∞–≤—Ç–æ–±—É—Å", "–º–æ—Ç–æ—Ü–∏–∫–ª"]
    
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å
    extractor = ImprovedRuleExtractor()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞
    rules = extractor.extract_semantic_rules(
        attention_weights, 
        text_tokens, 
        class_names=class_names,
        head_idx=0
    )
    
    print(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(rules)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª")
    print()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞
    for i, rule in enumerate(rules):
        print(f"üîπ –ü—Ä–∞–≤–∏–ª–æ {i+1}:")
        print(f"   ID: {rule.rule_id}")
        print(f"   –¢–∏–ø: {rule.semantic_type}")
        print(f"   –£—Å–ª–æ–≤–∏–µ: {rule.condition_text}")
        print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {rule.condition_image}")
        print(f"   –ó–∞–∫–ª—é—á–µ–Ω–∏–µ: {rule.conclusion}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {rule.confidence:.1%}")
        print(f"   –°–∏–ª–∞: {rule.strength:.3f}")
        print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {rule.linguistic_description}")
        print()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É
    summary = extractor.generate_rule_summary(rules)
    print("üìà –°–≤–æ–¥–∫–∞ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∞–≤–∏–ª: {summary['total_rules']}")
    print(f"   –¢–∏–ø—ã –ø—Ä–∞–≤–∏–ª: {summary['rule_types']}")
    print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {summary['avg_confidence']:.1%}")
    print(f"   –°—Ä–µ–¥–Ω—è—è —Å–∏–ª–∞: {summary['avg_strength']:.3f}")
    print(f"   {summary['summary']}")
    
    return rules, summary

if __name__ == "__main__":
    demo_improved_rule_extraction()
